# backend/app/services/adapters/hf_llm_adapter.py
# =================================================================
#
#                 Hugging Face LLM Adapter
#
# =================================================================
#
#  Purpose:
#  --------
#  Concrete implementation of the BaseLLMAdapter for Hugging Face
#  models. This adapter uses the `transformers` pipeline to interact
#  with a specified text-generation model.
#
#  Key Features:
#  -------------
#  - Initializes a Hugging Face text-generation pipeline.
#  - Implements the `generate_response` method to produce text.
#  - Uses `asyncio.to_thread` to run the synchronous pipeline in an
#    async-safe manner.
#
# =================================================================

import asyncio

from app.core.config import settings
from app.services.base.llm_adapter import BaseLLMAdapter
from transformers import pipeline


class HuggingFaceLLMAdapter(BaseLLMAdapter):
    """Adapter for Hugging Face text-generation models."""

    def __init__(self, model_name: str = "HuggingFaceH4/zephyr-7b-beta"):
        self.pipeline = pipeline(
            "text-generation",
            model=model_name,
            token=settings.HF_API_TOKEN,
        )

    async def generate_response(self, prompt: str) -> str:
        """
        Generates a response from the Hugging Face model.

        Args:
            prompt: The input text to send to the model.

        Returns:
            The text response generated by the model.
        """
        try:
            # The pipeline is synchronous, so we run it in a separate thread
            # to avoid blocking the asyncio event loop.
            result = await asyncio.to_thread(self.pipeline, prompt, max_new_tokens=150)
            return result[0]["generated_text"]
        except Exception as e:
            print(f"Error generating response from HuggingFace: {e}")
            return "Sorry, I encountered an error while generating a response."
