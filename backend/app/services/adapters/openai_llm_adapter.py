# backend/app/services/adapters/openai_llm_adapter.py
# =================================================================
#
#                    OpenAI LLM Adapter
#
# =================================================================
#
#  Purpose:
#  --------
#  Concrete implementation of the BaseLLMAdapter for OpenAI's GPT
#  models. This adapter uses the `openai` Python client to make
#  asynchronous API calls.
#
#  Key Features:
#  -------------
#  - Uses `openai.AsyncOpenAI` for non-blocking API requests.
#  - Implements the `generate_response` method.
#  - Configured via environment variables for the API key.
#
# =================================================================

from app.core.config import settings
from app.services.base.llm_adapter import BaseLLMAdapter
from openai import AsyncOpenAI


class OpenAILLMAdapter(BaseLLMAdapter):
    """Adapter for OpenAI's GPT models."""

    def __init__(self, model_name: str = "gpt-3.5-turbo"):
        self.model_name = model_name
        self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)

    async def generate_response(self, prompt: str) -> str:
        """
        Generates a response from the OpenAI model.

        Args:
            prompt: The input text to send to the model.

        Returns:
            The text response generated by the model.
        """
        try:
            response = await self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=150,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Error generating response from OpenAI: {e}")
            return "Sorry, I encountered an error with the AI model."
